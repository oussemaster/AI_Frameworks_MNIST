# mnist_tensorflow.py

# Importe der ben√∂tigten Bibliotheken
import tensorflow as tf  # Haupt-Framework f√ºr Deep Learning
from tensorflow.keras.datasets import mnist  # MNIST Datensatz
import tensorflow_datasets as tfds  # Alternative Datensatz-Quelle
import numpy as np  # Numerische Berechnungen
import matplotlib.pyplot as plt  # Visualisierung
import random  # Zufallsoperationen
import os  # Betriebssystem-Funktionen

# =============================================================================
# 1. DATENLADEN - Der erste Schritt im ML Workflow
# =============================================================================
print("\n1. üì• DATENS√ÑTZE LADEN")

# üîπ KRITISCHER KI-STEP: Daten acquisition
# Laden des MNIST-Datensatzes aus Keras
# R√ºckgabe: Tuple von (Trainingsdaten, Testdaten) jeweils mit (Images, Labels)
(x_train_keras, y_train_keras), (x_test_keras, y_test_keras) = mnist.load_data()
print(f"‚úÖ Keras: {x_train_keras.shape[0]:,} Trainingsbilder, {x_test_keras.shape[0]:,} Testbilder")

# üîπ ALTERNATIVE DATENQUELLE: TensorFlow Datasets
# Parameter: 
# - 'mnist': Name des Datensatzes
# - split: Aufteilung in Train/Test
# - as_supervised: Gibt (image, label) Tuple zur√ºck
# - shuffle_files: Mischt die Dateien beim Laden
ds_train_tfds, ds_test_tfds = tfds.load('mnist', split=['train', 'test'], 
                                       as_supervised=True, shuffle_files=True)
print(f"‚úÖ TFDS: Datens√§tze erfolgreich geladen")

# =============================================================================
# 2. DATENNORMALISIERUNG - Wichtiger Preprocessing Schritt
# =============================================================================
print("\n2. üîß DATENVORBEREITUNG")

def normalize_data(images: np.ndarray) -> np.ndarray:
    """
    Normalisiert Bilddaten auf den Wertebereich [0, 1].
    
    Parameter: images : np.ndarray -> Eingabebilder mit Pixelwerten im Bereich [0, 255]
    R√ºckgabe: np.ndarray -> Normalisierte Bilder im Bereich [0.0, 1.0] als float32
    """
    return images.astype('float32') / 255.0

# üîπ WICHTIGER KI-STEP: Daten-Normalisierung
# Warum? Neuronale Netze lernen besser mit normalisierten Daten
# Verhindert numerische Instabilit√§t und beschleunigt Training
x_train_norm = normalize_data(x_train_keras)
x_test_norm = normalize_data(x_test_keras)

print(f"üìä Wertebereich vor Normalisierung: [{x_train_keras.min()}, {x_train_keras.max()}]")
print(f"üìä Wertebereich nach Normalisierung: [{x_train_norm.min():.3f}, {x_train_norm.max():.3f}]")

# =============================================================================
# 3. DATENEXPLORATION - Verstehen der Datenverteilung
# =============================================================================
print("\n3. üìä DATENSATZ-STATISTIKEN")

def print_dataset_stats(images: np.ndarray, labels: np.ndarray, name: str) -> None:
    """
    Gibt detaillierte Statistiken √ºber den Datensatz aus.
    
    Parameter:
    ----------
    images : np.ndarray -> Array mit Bilddaten
    labels : np.ndarray -> Array mit Labels/Klassen
    name : str -> Bezeichnung des Datensatzes f√ºr die Ausgabe
    R√ºckgabe:  None
    """
    print(f"\n--- {name} ---")
    print(f"Shape: {images.shape}")  # Dimensionen der Daten
    print(f"Datentyp: {images.dtype}")  # Datentyp der Pixelwerte
    print(f"Eindeutige Klassen: {np.unique(labels)}")  # Verf√ºgbare Labels
    
    # üîπ WICHTIGER KI-STEP: Klassenverteilungsanalyse
    # Stellt sicher, dass alle Klassen gleichm√§√üig vertreten sind
    unique, counts = np.unique(labels, return_counts=True)
    print("Klassenverteilung:")
    for label, count in zip(unique, counts):
        print(f"  Ziffer {label}: {count:>5} Beispiele ({count/len(labels)*100:.1f}%)")

# Statistiken f√ºr beide Datens√§tze ausgeben
print_dataset_stats(x_train_norm, y_train_keras, "TRAININGSDATEN")
print_dataset_stats(x_test_norm, y_test_keras, "TESTDATEN")

# =============================================================================
# 4. DATENVISUALISIERUNG - Qualitative Datenanalyse
# =============================================================================
print("\n4. üñºÔ∏è VISUALISIERUNG ZUF√ÑLLIGER BILDER")

def plot_random_samples(images: np.ndarray, labels: np.ndarray, num_samples: int = 10) -> None:
    """
    Erstellt eine Visualisierung von zuf√§llig ausgew√§hlten Beispielbildern.
    
    Parameter:
    ----------
    images : np.ndarray
        Array mit Bilddaten
    labels : np.ndarray
        Array mit Labels
    num_samples : int, optional
        Anzahl der anzuzeigenden Beispiele (default: 10)
    
    R√ºckgabe:
    --------
    None
    """
    # üîπ Zuf√§llige Auswahl f√ºr repr√§sentative Stichprobe
    indices = random.sample(range(len(images)), num_samples)
    
    # Erstelle Subplots mit 2 Reihen und 5 Spalten
    fig, axes = plt.subplots(2, 5, figsize=(15, 6))
    axes = axes.ravel()  # Mache das 2D-Array zu 1D f√ºr einfache Iteration
    
    for i, idx in enumerate(indices):
        axes[i].imshow(images[idx], cmap='gray')  # Bild anzeigen
        axes[i].set_title(f'Label: {labels[idx]}', fontsize=12, weight='bold')
        axes[i].axis('off')  # Achsen ausblenden
    
    plt.tight_layout()
    plt.savefig('mnist_random_samples.png', dpi=150, bbox_inches='tight')
    print(f"‚úÖ Zuf√§llige Beispiele gespeichert als 'mnist_random_samples.png'")

# üîπ WICHTIGER KI-STEP: Qualitative Datenpr√ºfung
# Sicherstellen, dass Daten korrekt geladen und Labels stimmen
plot_random_samples(x_train_norm, y_train_keras)

# =============================================================================
# 5. KLASSENSPEZIFISCHE ANALYSE - Verstehen jeder einzelnen Klasse
# =============================================================================
print("\n5. üî¢ BEISPIEL F√úR JEDE ZIFFER")

def plot_one_per_class(images: np.ndarray, labels: np.ndarray) -> None:
    """
    Zeigt das erste Beispiel f√ºr jede Klasse (Ziffer 0-9) mit Pixelwerten.
    
    Parameter:
    ----------
    images : np.ndarray
        Array mit Bilddaten
    labels : np.ndarray
        Array mit Labels
    
    R√ºckgabe:
    --------
    None
    """
    fig, axes = plt.subplots(2, 5, figsize=(15, 6))
    axes = axes.ravel()
    
    for digit in range(10):
        # üîπ Finde ersten Index f√ºr jede Ziffer
        # np.where gibt Indices zur√ºck wo labels == digit
        idx = np.where(labels == digit)[0][0]
        
        axes[digit].imshow(images[idx], cmap='viridis')
        axes[digit].set_title(f'Ziffer: {digit}', fontsize=14, weight='bold', pad=10)
        axes[digit].axis('off')
        
        # üîπ DETAILIERTE PIXELANALYSE: Zeige Pixelwerte f√ºr besseres Verst√§ndnis
        height, width = images[idx].shape
        for i in range(height):
            for j in range(width):
                if images[idx][i, j] > 0.5:  # Nur helle Pixel beschriften f√ºr Lesbarkeit
                    axes[digit].text(j, i, f'{images[idx][i, j]:.1f}', 
                                   ha='center', va='center', fontsize=6, 
                                   color='white', alpha=0.7)
    
    plt.suptitle('Erstes Beispiel f√ºr jede Ziffer 0-9 mit Pixelwerten', 
                 fontsize=16, weight='bold', y=1.02)
    plt.tight_layout()
    plt.savefig('mnist_per_class.png', dpi=150, bbox_inches='tight')
    print("‚úÖ Beispiele pro Klasse gespeichert als 'mnist_per_class.png'")

plot_one_per_class(x_train_norm, y_train_keras)

# =============================================================================
# 6. DATENSATZ-VORBEREITUNG - Finale Vorbereitung f√ºr Training
# =============================================================================
print("\n6. üéØ DATENS√ÑTZE F√úR TRAINING VORBEREITEN")

def create_tf_dataset(images: np.ndarray, labels: np.ndarray, 
                     batch_size: int = 32, shuffle: bool = True) -> tf.data.Dataset:
    """
    Erstellt einen optimierten TensorFlow Dataset f√ºr effizientes Training.
    
    Parameter:
    ----------
    images : np.ndarray
        Array mit Bilddaten
    labels : np.ndarray
        Array mit Labels
    batch_size : int, optional
        Gr√∂√üe der Batches (default: 32)
    shuffle : bool, optional
        Ob die Daten gemischt werden sollen (default: True)
    
    R√ºckgabe:
    --------
    tf.data.Dataset
        Optimierter TensorFlow Dataset
    """
    # üîπ WICHTIGER KI-STEP: TensorFlow Dataset erstellen
    # from_tensor_slices: Erstellt Dataset aus numpy Arrays
    dataset = tf.data.Dataset.from_tensor_slices((images, labels))
    
    if shuffle:
        # üîπ MISCHEN: Verhindert, dass das Modell die Reihenfolge lernt
        dataset = dataset.shuffle(buffer_size=10000)
    
    # üîπ BATCHING: Teilt Daten in kleine Gruppen f√ºr effizientes Training
    dataset = dataset.batch(batch_size)
    
    # üîπ PREFETCHING: L√§dt n√§chste Batches parallel zum Training
    # AUTOTUNE: TensorFlow w√§hlt optimale Anzahl automatisch
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    
    return dataset

# Parameter f√ºr das Training
BATCH_SIZE = 64  # Anzahl Bilder pro Trainingsschritt

# üîπ WICHTIGER KI-STEP: Finale Datensatzerstellung
# Trainingsdatensatz: Gemischt f√ºr bessere Generalisierung
train_dataset = create_tf_dataset(x_train_norm, y_train_keras, 
                                 batch_size=BATCH_SIZE, shuffle=True)
# Testdatensatz: Nicht mischen f√ºr konsistente Evaluation
test_dataset = create_tf_dataset(x_test_norm, y_test_keras, 
                                batch_size=BATCH_SIZE, shuffle=False)

print(f"‚úÖ Trainingsdatensatz: Batch-Gr√∂√üe {BATCH_SIZE}, shuffling aktiviert")
print(f"‚úÖ Testdatensatz: Batch-Gr√∂√üe {BATCH_SIZE}, shuffling deaktiviert")

# =============================================================================
# 7. BATCH-VERIFIZIERUNG - Sicherstellen der korrekten Verarbeitung
# =============================================================================
print("\n7. üîÑ BATCH-VERARBEITUNG TESTEN")

def inspect_batches(dataset: tf.data.Dataset, num_batches: int = 1) -> None:
    """
    Untersucht die Struktur und Inhalte der Batches im Dataset.
    
    Parameter:
    ----------
    dataset : tf.data.Dataset
        TensorFlow Dataset zur Untersuchung
    num_batches : int, optional
        Anzahl der zu untersuchenden Batches (default: 1)
    
    R√ºckgabe:
    --------
    None
    """
    print("Erste Batch-Informationen:")
    
    # üîπ DATASET-ITERATION: take() nimmt die ersten n Batches
    for batch_idx, (batch_images, batch_labels) in enumerate(dataset.take(num_batches)):
        print(f"\nBatch {batch_idx + 1}:")
        print(f"  - Images Shape: {batch_images.shape}")  # [batch_size, 28, 28]
        print(f"  - Labels Shape: {batch_labels.shape}")  # [batch_size]
        print(f"  - Labels im Batch: {batch_labels.numpy()}")  # Tats√§chliche Labels
        print(f"  - Wertebereich: [{batch_images.numpy().min():.3f}, {batch_images.numpy().max():.3f}]")

# üîπ QUALIT√ÑTSKONTROLLE: Verifizieren der Batch-Verarbeitung
inspect_batches(train_dataset)

# =============================================================================
# 8. ZUSAMMENFASSUNG - Finaler Statusbericht
# =============================================================================
print("\n" + "=" * 50)
print("üéâ ZUSAMMENFASSUNG")
print("=" * 50)

print(f"‚úÖ MNIST Datensatz erfolgreich geladen und vorbereitet")
print(f"üìÅ Trainingsdaten: {x_train_norm.shape[0]:,} Bilder")
print(f"üìÅ Testdaten: {x_test_norm.shape[0]:,} Bilder") 
print(f"üìê Bildgr√∂√üe: {x_train_norm.shape[1]}x{x_train_norm.shape[2]} Pixel")
print(f"üéØ Anzahl Klassen: 10 (Ziffern 0-9)")
print(f"üì¶ Batch-Gr√∂√üe: {BATCH_SIZE}")
print(f"üíæ Dateien gespeichert: mnist_random_samples.png, mnist_per_class.png")

print("\nüöÄ Der Datensatz ist jetzt bereit f√ºr Deep Learning Modelle!")

# =============================================================================
# 9. KI-MODELL ERSTELLEN UND TRAINIEREN
# =============================================================================
print("\n9. üß† KI-MODELL TRAINIEREN")

def create_and_train_model(x_train, y_train, x_test, y_test):
    """
    Erstellt und trainiert ein CNN-Modell f√ºr die Ziffernerkennung.
    
    Parameter:
    ----------
    x_train : np.ndarray - Trainingsbilder
    y_train : np.ndarray - Trainingslabels  
    x_test : np.ndarray - Testbilder
    y_test : np.ndarray - Testlabels
    
    R√ºckgabe:
    --------
    tf.keras.Model - Trainiertes Modell
    dict - Trainingsverlauf
    """
    
    # üîπ WICHTIG: Daten f√ºr CNN vorbereiten (Channel Dimension hinzuf√ºgen)
    x_train_cnn = x_train.reshape(-1, 28, 28, 1)  # Shape: (60000, 28, 28, 1)
    x_test_cnn = x_test.reshape(-1, 28, 28, 1)    # Shape: (10000, 28, 28, 1)
    
    # üîπ Labels in kategorisches Format umwandeln (one-hot encoding)
    y_train_categorical = tf.keras.utils.to_categorical(y_train, 10)
    y_test_categorical = tf.keras.utils.to_categorical(y_test, 10)
    
    print("üìê Modell-Input Shape:", x_train_cnn.shape)
    print("üéØ Modell-Output Shape:", y_train_categorical.shape)
    
    # üîπ CONVOLUTIONAL NEURAL NETWORK (CNN) erstellen
    model = tf.keras.Sequential([
        # Erste Convolutional Layer - lernt grundlegende Muster (Kanten, Ecken)
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D((2, 2)),
        
        # Zweite Convolutional Layer - lernt komplexere Muster (Kreise, Kurven)
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D((2, 2)),
        
        # Dritte Convolutional Layer - lernt hochlevel Merkmale
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
        
        # üîπ Vektorisierung f√ºr Fully-Connected Layers
        tf.keras.layers.Flatten(),
        
        # üîπ Fully-Connected Layers f√ºr Klassifikation
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dropout(0.5),  # Verhindert Overfitting
        
        # üîπ Output Layer - 10 Neuronen f√ºr 10 Ziffern (0-9)
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
    # üîπ MODELL KOMPILIEREN
    model.compile(
        optimizer='adam',           # Optimierungsalgorithmus
        loss='categorical_crossentropy',  # Verlustfunktion f√ºr Klassifikation
        metrics=['accuracy']        # Metrik zur Leistungsbewertung
    )
    
    print("‚úÖ Modell erfolgreich kompiliert")
    print(model.summary())
    
    # üîπ MODELL TRAINIEREN
    print("üöÄ Starte Training...")
    history = model.fit(
        x_train_cnn, y_train_categorical,
        epochs=10,                  # Anzahl der Trainingsdurchl√§ufe
        batch_size=32,              # Bilder pro Update
        validation_split=0.2,       # 20% f√ºr Validation
        verbose=1
    )
    
    # üîπ MODELL EVALUIEREN
    test_loss, test_accuracy = model.evaluate(x_test_cnn, y_test_categorical, verbose=0)
    print(f"üéØ Test-Genauigkeit: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
    
    return model, history

# Modell erstellen und trainieren
trained_model, training_history = create_and_train_model(
    x_train_norm, y_train_keras, 
    x_test_norm, y_test_keras
)

# Modell speichern f√ºr sp√§tere Verwendung
trained_model.save('mnist_cnn_model.h5')
print("üíæ Modell gespeichert als 'mnist_cnn_model.h5'")

# =============================================================================
# 10. VORHERSAGE F√úR NEUE BILDER
# =============================================================================
print("\n10. üîÆ VORHERSAGE F√úR EIGENE BILDER")

def predict_custom_image(model, image_path):
    """
    Macht eine Vorhersage f√ºr ein eigenes Zahlenbild.
    
    Parameter:
    ----------
    model : tf.keras.Model - Trainiertes Modell
    image_path : str - Pfad zum Bild
    
    R√ºckgabe:
    --------
    int - Vorhergesagte Ziffer (0-9)
    np.ndarray - Wahrscheinlichkeiten f√ºr alle Ziffern
    """
    
    try:
        # üîπ Bild laden und vorbereiten
        from PIL import Image
        import numpy as np
        
        # Bild in Graustufen laden und auf 28x28 skalieren
        img = Image.open(image_path).convert('L')  # Zu Graustufen
        img = img.resize((28, 28))                 # Auf MNIST-Gr√∂√üe skalieren
        
        # Zu Numpy-Array konvertieren
        img_array = np.array(img)
        
        # üîπ WICHTIG: Invertieren falls n√∂tig (MNIST ist wei√ü auf schwarz)
        # Wenn dein Bild schwarze Ziffern auf wei√üem Hintergrund hat:
        if np.mean(img_array) > 127:  # Heller Hintergrund
            img_array = 255 - img_array  # Invertieren
        
        # Normalisieren und f√ºr Modell vorbereiten
        img_array = img_array.astype('float32') / 255.0
        img_array = img_array.reshape(1, 28, 28, 1)  # Batch-Dimension hinzuf√ºgen
        
        # üîπ VORHERSAGE MACHT
        predictions = model.predict(img_array, verbose=0)
        predicted_digit = np.argmax(predictions[0])
        confidence = np.max(predictions[0])
        
        print(f"üîç Vorhersage f√ºr {image_path}:")
        print(f"   üéØ Ziffer: {predicted_digit}")
        print(f"   üìä Sicherheit: {confidence:.2%}")
        
        # Zeige Top-3 Vorhersagen
        top_3_indices = np.argsort(predictions[0])[-3:][::-1]
        print("   üìà Top-3 Vorhersagen:")
        for i, idx in enumerate(top_3_indices):
            print(f"      {i+1}. Ziffer {idx}: {predictions[0][idx]:.2%}")
        
        return predicted_digit, predictions[0]
        
    except Exception as e:
        print(f"‚ùå Fehler beim Verarbeiten des Bildes: {e}")
        return None, None

# Beispiel f√ºr die Verwendung:
# predicted, probs = predict_custom_image(trained_model, 'meine_zahl.png')

# =============================================================================
# 11. EINFACHE BENUTZEROBERFL√ÑCHE
# =============================================================================
print("\n11. üñ±Ô∏è BENUTZEROBERFL√ÑCHE")

def interactive_prediction():
    """
    Bietet eine interaktive M√∂glichkeit, eigene Bilder zu klassifizieren.
    """
    print("\n" + "="*50)
    print("INTERAKTIVE ZIFFERNERKENNUNG")
    print("="*50)
    print("So verwendest du das Modell:")
    print("1. Erstelle ein Bild einer handgeschriebenen Ziffer (0-9)")
    print("2. Speichere es als PNG/JPG (empfohlen: 28x28 Pixel)")
    print("3. Gib den Pfad zum Bild ein")
    print("4. Das Modell sagt die Ziffer vorher!")
    print("\nTipp: Verwende Paint, Photoshop oder zeichne auf deinem Handy")
    
    while True:
        print("\n--- Neue Vorhersage ---")
        image_path = input("Pfad zum Bild (oder 'quit' zum Beenden): ").strip()
        
        if image_path.lower() == 'quit':
            break
            
        if not os.path.exists(image_path):
            print("‚ùå Datei existiert nicht! Bitte Pfad √ºberpr√ºfen.")
            continue
            
        # Vorhersage machen
        predicted, probabilities = predict_custom_image(trained_model, image_path)
        
        if predicted is not None:
            # Bild anzeigen
            img = Image.open(image_path)
            plt.figure(figsize=(6, 6))
            plt.imshow(img, cmap='gray')
            plt.title(f'Vorhersage: Ziffer {predicted}', fontsize=16, weight='bold')
            plt.axis('off')
            plt.show()

# üîπ BENUTZEROBERFL√ÑCHE STARTEN (auskommentiert, da sie Input ben√∂tigt)
# interactive_prediction()

